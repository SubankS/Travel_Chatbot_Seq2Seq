{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #regular expression operations\n",
    "import random #generate pseudo random numbers\n",
    "import numpy as np #array operations\n",
    "from nltk.tokenize import word_tokenize #tokenizer -> converts string to tokens\n",
    "from keras.callbacks import History\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# modules needed for building seq2seq model\n",
    "from tensorflow import keras # Keras is an open-source library that provides a Python interface for artificial neural networks.\n",
    "from keras.layers import Input # Layer to be used as an entry point into a Network (a graph of layers).\n",
    "from keras.layers import LSTM # Long Short-Term Memory layer\n",
    "from keras.layers import Dense # regular densely-connected NN layer.\n",
    "from keras.models import Model # Model groups layers into an object with training and inference features.\n",
    "from keras.models import load_model # Loads a model saved via model.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the data files . We have two separate .txt files which stores human_responses and bot_responses\n",
    "# l1 contains human_responses , l2 contains bot_responses\n",
    "# UTF-8 is one of the most commonly used encodings, and Python often defaults to using it.\n",
    "with open(r'human_responses_1084.txt',encoding='utf-8') as file: \n",
    "    l1 = file.read().split('\\n') #splitting at newline  \n",
    "with open(r'bot_responses_1084.txt',encoding='utf-8') as file:\n",
    "    l2 = file.read().split('\\n') #splitting at newline\n",
    "    \n",
    "# Here we are using regular expressions to remove punctuations (except comma), special symbols\n",
    "# We keep comma because it helps to separate multiple proper nouns\n",
    "# Converting whole sentences to lower case to reduce ambiguity\n",
    "l1 = [re.sub('[^,a-zA-Z0-9 \\n]', '', x) for x in l1]\n",
    "l1 = [x.lower() for x in l1]\n",
    "l2 = [re.sub('[^,a-zA-Z0-9 \\n]', '', x) for x in l2]\n",
    "l2 = [x.lower() for x in l2]\n",
    "\n",
    "# zip produces a tuple of (human_response,bot_response)\n",
    "# then we convert it into a list of tuples\n",
    "lis = list(zip(l1,l2))\n",
    "\n",
    "# Finding all unique input tokens\n",
    "HumanResponses = [] #stores the human responses as a list - also our input lines\n",
    "InputTokens = set() #stores unique input tokens . We took a set because set has the feature of storing all unique elements\n",
    "for y in lis:\n",
    "    a = y[0] #y[0]->human\n",
    "    # Spacing words from punctuation (so that comma can be treated as an unique token)\n",
    "    # basically finds all unique tokens and appends them with spaces in between\n",
    "    a = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", a))\n",
    "    # Appending each human side sentence to human_responses\n",
    "    HumanResponses.append(a)\n",
    "    # Now we tokenize each sentence into tokens and add each unique token to our token set\n",
    "    # We are using word_tokenize() from NLTK\n",
    "    for t in word_tokenize(a):\n",
    "        if t not in InputTokens:\n",
    "            InputTokens.add(t)\n",
    "InputTokens = sorted(list(InputTokens)) # converting set to list and sorting it in alphabetical order\n",
    "EncoderTokenLen = len(InputTokens) #no of input tokens\n",
    "\n",
    "# Finding all unique output tokens\n",
    "BotResponses = [] #stores the bot responses as a list - also our output lines\n",
    "OutputTokens = set() #stores unique output tokens. We took a set because set has the feature of storing all unique elements\n",
    "for y in lis:\n",
    "    a = y[1] #y[1]->bot\n",
    "    # Spacing words from punctuation (so that comma can be treated as an unique token)\n",
    "    # basically finds all unique tokens and appends them with spaces in between\n",
    "    a = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", a))\n",
    "    # Manipulating a by adding tracker tokens (<START> & <STOP>) and appending it to bot_responses\n",
    "    # START -> helps the decoder to know what is the starting of a sentence\n",
    "    # STOP -> helps the decoder to know that is the ending of a sentence\n",
    "    a = 'START ' + a + ' STOP'\n",
    "    BotResponses.append(a)\n",
    "    # Now we tokenize each sentence into tokens and add each unique token to our token set\n",
    "    # We are using word_tokenize() from NLTK\n",
    "    for t in word_tokenize(a):\n",
    "        if t not in OutputTokens:\n",
    "            OutputTokens.add(t)\n",
    "OutputTokens = sorted(list(OutputTokens))\n",
    "DecoderTokenLen = len(OutputTokens) #no of output tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate is a function which returns the count/index of elements in a list along with the elements\n",
    "# Feature dictionaries will help us to vectorize text (process of converting text into numerical representation) . We will use one-hot vectorization\n",
    "# Reverse feature dictionaries will help us to reverse vectorize text (process of converting vectorized text to text)\n",
    "# Dictionary containing pairs in the format (key : token , value : index) for the input vocabulary\n",
    "InputFeatures_Dict = {}\n",
    "for i,t in enumerate(InputTokens):\n",
    "    InputFeatures_Dict.update({t:i})\n",
    "# Dictionary containing pairs in the format (key : index , value : token) for the input vocabulary . Reverse of input features dict\n",
    "RevInputFeatures_Dict = {}\n",
    "for i,t in enumerate(InputTokens):\n",
    "    RevInputFeatures_Dict.update({i:t})\n",
    "# Dictionary containing pairs in the format (key : token , value : index) for the output vocabulary\n",
    "OutputFeatures_Dict = {}\n",
    "for i,t in enumerate(OutputTokens):\n",
    "    OutputFeatures_Dict.update({t:i})\n",
    "# Dictionary containing pairs in the format (key : index , value : token) for the output vocabulary . Reverse of output features dict\n",
    "RevOutputFeatures_Dict = {}\n",
    "for i,t in enumerate(OutputTokens):\n",
    "    RevOutputFeatures_Dict.update({i:t})\n",
    "\n",
    "#Maximum no of tokens in a single sentence in input and output documents. Basically helps in padding\n",
    "MaxEncoder_Len = 0\n",
    "for x in HumanResponses:\n",
    "    MaxEncoder_Len = max(len(word_tokenize(x)),MaxEncoder_Len)\n",
    "MaxDecoder_Len = 0\n",
    "for x in BotResponses:\n",
    "    MaxDecoder_Len = max(len(word_tokenize(x)),MaxDecoder_Len)\n",
    "\n",
    "\n",
    "# We will make use of 3 matrices EncoderInput_data, DecoderInput_data, DecoderOutput_data. (Encoder output is nothing but the decoder input)\n",
    "# The reason we are using two matrices for the Decoder is a method called teacher forcing which is used by the seq2seq model while training.\n",
    "# TEACHER FORCING - https://towardsdatascience.com/what-is-teacher-forcing-3da6217fed1c\n",
    "\n",
    "# One-hot vectorization taking place\n",
    "# 3D matrix - total no of sentences x max token length of a sentence from the corpus x total no of unique tokens\n",
    "\n",
    "# Filling EncoderInput_data\n",
    "EncoderInput_data = np.zeros((len(HumanResponses), MaxEncoder_Len, EncoderTokenLen),dtype='float32')\n",
    "for l,(a,b) in enumerate(zip(HumanResponses,BotResponses)): # returns a tuple along with index\n",
    "    for ts,t in enumerate(word_tokenize(a)): # human responses are the encoder input\n",
    "        # Assign 1.0 for the current sentence, timestep, & token in EncoderInput_data\n",
    "        # l -> sentence number, ts -> timestep, InputFeatures_Dict[t] -> index value of the token t which was extracted from unique token list\n",
    "        EncoderInput_data[l,ts,InputFeatures_Dict[t]] = 1.0\n",
    "        \n",
    "# Filling DecoderInput_data        \n",
    "DecoderInput_data = np.zeros((len(BotResponses), MaxDecoder_Len, DecoderTokenLen),dtype='float32')\n",
    "for l,(a,b) in enumerate(zip(HumanResponses,BotResponses)): # returns a tuple along with index\n",
    "    for ts,t in enumerate(word_tokenize(b)):\n",
    "        # Assign 1.0 for the current sentence, timestep, & token in DecoderInput_data\n",
    "        # l -> sentence number, ts -> timestep, InputFeatures_Dict[t] -> index value of the token t which was extracted from unique token list\n",
    "        DecoderInput_data[l,ts,OutputFeatures_Dict[t]] = 1.0\n",
    "        \n",
    "# Filling DecoderOutput_data\n",
    "DecoderOutput_data = np.zeros((len(BotResponses), MaxDecoder_Len, DecoderTokenLen),dtype='float32')\n",
    "for l,(a,b) in enumerate(zip(HumanResponses,BotResponses)): # returns a tuple along with index\n",
    "    for ts,t in enumerate(word_tokenize(b)):\n",
    "        if ts > 0:\n",
    "            # DecoderOutput_data helps us in implementing teacher forcing\n",
    "            DecoderOutput_data[l,ts-1,OutputFeatures_Dict[t]] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 867 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 1.3074 - accuracy: 0.6437 - val_loss: 1.3355 - val_accuracy: 0.7737\n",
      "Epoch 2/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 1.1706 - accuracy: 0.6929 - val_loss: 1.2299 - val_accuracy: 0.7744\n",
      "Epoch 3/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 1.0785 - accuracy: 0.7820 - val_loss: 1.1386 - val_accuracy: 0.7776\n",
      "Epoch 4/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.9882 - accuracy: 0.7823 - val_loss: 1.0417 - val_accuracy: 0.7661\n",
      "Epoch 5/50\n",
      "867/867 [==============================] - 6s 6ms/step - loss: 0.8602 - accuracy: 0.7764 - val_loss: 0.9056 - val_accuracy: 0.7494\n",
      "Epoch 6/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.6848 - accuracy: 0.7276 - val_loss: 0.6923 - val_accuracy: 0.7564\n",
      "Epoch 7/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.5088 - accuracy: 0.6178 - val_loss: 0.4672 - val_accuracy: 0.4920\n",
      "Epoch 8/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.3391 - accuracy: 0.5457 - val_loss: 0.3061 - val_accuracy: 0.8126\n",
      "Epoch 9/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.2165 - accuracy: 0.6531 - val_loss: 0.2018 - val_accuracy: 0.6002\n",
      "Epoch 10/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.1327 - accuracy: 0.6349 - val_loss: 0.1205 - val_accuracy: 0.7319\n",
      "Epoch 11/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0925 - accuracy: 0.6289 - val_loss: 0.0868 - val_accuracy: 0.6444\n",
      "Epoch 12/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0707 - accuracy: 0.6109 - val_loss: 0.0682 - val_accuracy: 0.6617\n",
      "Epoch 13/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0586 - accuracy: 0.6247 - val_loss: 0.0577 - val_accuracy: 0.6687\n",
      "Epoch 14/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0509 - accuracy: 0.6172 - val_loss: 0.0509 - val_accuracy: 0.6522\n",
      "Epoch 15/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0460 - accuracy: 0.6110 - val_loss: 0.0470 - val_accuracy: 0.6239\n",
      "Epoch 16/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0427 - accuracy: 0.6119 - val_loss: 0.0440 - val_accuracy: 0.6253\n",
      "Epoch 17/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0406 - accuracy: 0.6088 - val_loss: 0.0422 - val_accuracy: 0.6203\n",
      "Epoch 18/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0392 - accuracy: 0.6063 - val_loss: 0.0406 - val_accuracy: 0.6290\n",
      "Epoch 19/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0381 - accuracy: 0.6349 - val_loss: 0.0395 - val_accuracy: 0.6393\n",
      "Epoch 20/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0370 - accuracy: 0.6205 - val_loss: 0.0385 - val_accuracy: 0.6431\n",
      "Epoch 21/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0362 - accuracy: 0.6269 - val_loss: 0.0379 - val_accuracy: 0.6323\n",
      "Epoch 22/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0356 - accuracy: 0.6213 - val_loss: 0.0372 - val_accuracy: 0.6343\n",
      "Epoch 23/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0352 - accuracy: 0.6268 - val_loss: 0.0368 - val_accuracy: 0.6330\n",
      "Epoch 24/50\n",
      "867/867 [==============================] - 6s 6ms/step - loss: 0.0348 - accuracy: 0.6270 - val_loss: 0.0370 - val_accuracy: 0.6249\n",
      "Epoch 25/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0348 - accuracy: 0.6285 - val_loss: 0.0363 - val_accuracy: 0.6422\n",
      "Epoch 26/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0344 - accuracy: 0.6277 - val_loss: 0.0365 - val_accuracy: 0.6312\n",
      "Epoch 27/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0341 - accuracy: 0.6264 - val_loss: 0.0355 - val_accuracy: 0.6402\n",
      "Epoch 28/50\n",
      "867/867 [==============================] - 6s 6ms/step - loss: 0.0340 - accuracy: 0.6200 - val_loss: 0.0353 - val_accuracy: 0.6454\n",
      "Epoch 29/50\n",
      "867/867 [==============================] - 6s 6ms/step - loss: 0.0337 - accuracy: 0.6254 - val_loss: 0.0350 - val_accuracy: 0.6496\n",
      "Epoch 30/50\n",
      "867/867 [==============================] - 6s 6ms/step - loss: 0.0335 - accuracy: 0.6254 - val_loss: 0.0349 - val_accuracy: 0.6537\n",
      "Epoch 31/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0333 - accuracy: 0.6273 - val_loss: 0.0347 - val_accuracy: 0.6514\n",
      "Epoch 32/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0331 - accuracy: 0.6263 - val_loss: 0.0343 - val_accuracy: 0.6536\n",
      "Epoch 33/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0330 - accuracy: 0.6266 - val_loss: 0.0342 - val_accuracy: 0.6514\n",
      "Epoch 34/50\n",
      "867/867 [==============================] - 6s 6ms/step - loss: 0.0325 - accuracy: 0.6277 - val_loss: 0.0336 - val_accuracy: 0.6608\n",
      "Epoch 35/50\n",
      "867/867 [==============================] - 6s 6ms/step - loss: 0.0323 - accuracy: 0.6202 - val_loss: 0.0339 - val_accuracy: 0.6504\n",
      "Epoch 36/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0323 - accuracy: 0.6210 - val_loss: 0.0327 - val_accuracy: 0.6576\n",
      "Epoch 37/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0294 - accuracy: 0.6264 - val_loss: 0.0312 - val_accuracy: 0.6607\n",
      "Epoch 38/50\n",
      "867/867 [==============================] - 6s 6ms/step - loss: 0.0277 - accuracy: 0.6252 - val_loss: 0.0292 - val_accuracy: 0.6089\n",
      "Epoch 39/50\n",
      "867/867 [==============================] - 6s 6ms/step - loss: 0.0247 - accuracy: 0.6157 - val_loss: 0.0269 - val_accuracy: 0.6770\n",
      "Epoch 40/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0349 - accuracy: 0.5962 - val_loss: 0.2925 - val_accuracy: 0.2255\n",
      "Epoch 41/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0566 - accuracy: 0.5021 - val_loss: 0.0264 - val_accuracy: 0.6141\n",
      "Epoch 42/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0192 - accuracy: 0.6149 - val_loss: 0.0199 - val_accuracy: 0.6554\n",
      "Epoch 43/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0142 - accuracy: 0.6169 - val_loss: 0.0151 - val_accuracy: 0.6506\n",
      "Epoch 44/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0102 - accuracy: 0.6419 - val_loss: 0.0120 - val_accuracy: 0.6608\n",
      "Epoch 45/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0079 - accuracy: 0.6303 - val_loss: 0.0098 - val_accuracy: 0.6536\n",
      "Epoch 46/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0082 - accuracy: 0.6347 - val_loss: 0.0176 - val_accuracy: 0.6501\n",
      "Epoch 47/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0115 - accuracy: 0.4786 - val_loss: 0.0112 - val_accuracy: 0.3072\n",
      "Epoch 48/50\n",
      "867/867 [==============================] - 6s 7ms/step - loss: 0.0080 - accuracy: 0.4807 - val_loss: 0.0086 - val_accuracy: 0.5414\n",
      "Epoch 49/50\n",
      "867/867 [==============================] - 6s 6ms/step - loss: 0.0039 - accuracy: 0.5605 - val_loss: 0.0057 - val_accuracy: 0.5863\n",
      "Epoch 50/50\n",
      "867/867 [==============================] - 6s 6ms/step - loss: 0.0024 - accuracy: 0.5533 - val_loss: 0.0037 - val_accuracy: 0.5791\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "#Dimensionality -> no of LSTM units\n",
    "dimensionality = 512 \n",
    "#The batch size and number of epochs\n",
    "batch_size = 10\n",
    "epochs = 50\n",
    "#Encoder\n",
    "EncoderInput = Input(shape=(None,EncoderTokenLen)) \n",
    "EncoderLSTM = LSTM(dimensionality, return_state=True)\n",
    "EncoderOutput,HiddenState_En,StateCell_En = EncoderLSTM(EncoderInput)\n",
    "EncoderStates = [HiddenState_En,StateCell_En]\n",
    "#Decoder\n",
    "DecoderInput = Input(shape=(None,DecoderTokenLen))\n",
    "DecoderLSTM = LSTM(dimensionality, return_sequences=True, return_state=True)\n",
    "DecoderOutput,HiddenState_De,StateCell_De = DecoderLSTM(DecoderInput,initial_state=EncoderStates)\n",
    "DecoderDense = Dense(DecoderTokenLen,activation='softmax')\n",
    "DecoderOutput = DecoderDense(DecoderOutput)\n",
    "\n",
    "#Model\n",
    "Train_model = Model([EncoderInput,DecoderInput], DecoderOutput)\n",
    "#Compiling\n",
    "Train_model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'], sample_weight_mode='temporal')\n",
    "#Training\n",
    "history = History()\n",
    "Train_model.fit([EncoderInput_data,DecoderInput_data],DecoderOutput_data,batch_size = batch_size,epochs = epochs,validation_split = 0.2, shuffle=False,callbacks=[history])\n",
    "Train_model.save('Train_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_model = load_model('Train_model.h5')\n",
    "EncoderInput = Train_model.input[0]\n",
    "EncoderOutput,state_h_En,state_c_En = Train_model.layers[2].output\n",
    "EncoderStates = [state_h_En, state_c_En]\n",
    "EncoderModel = Model(EncoderInput,EncoderStates)\n",
    "\n",
    "latent_dim = 512\n",
    "DecoderStateInput_hid = Input(shape=(latent_dim,))\n",
    "DecoderStateInput_cell = Input(shape=(latent_dim,))\n",
    "DecoderStateInput = [DecoderStateInput_hid,DecoderStateInput_cell]\n",
    "\n",
    "DecoderOutput,HiddenState_En,StateCell_En = DecoderLSTM(DecoderInput,initial_state=DecoderStateInput)\n",
    "DecoderState = [HiddenState_En,StateCell_En]\n",
    "DecoderOutput = DecoderDense(DecoderOutput)\n",
    "\n",
    "DecoderModel = Model([DecoderInput] + DecoderStateInput,[DecoderOutput] + DecoderState)\n",
    "\n",
    "\n",
    "negative_word = (\"no\", \"nope\", \"nah\", \"naw\", \"not a chance\", \"sorry\", \"never\") # list of negative responses\n",
    "exit_word = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\") # list of exit cues\n",
    "exclamation_word = (\"okay\",\"thanks\",\"thank you\",\"okay nice\",\"okay nice to know\",\"nice\") # list of exclamatory responses\n",
    "\n",
    "def Initiate():\n",
    "    user_input = input(\"Hi, I'm Kolly. I can help you with Kolkata tourism. Ask me about historical locations & restaurants. Type `bye` to exit me. What would you like to know?\\n\")\n",
    "    if user_input in negative_word:\n",
    "        print(\"okay , have a great day\")\n",
    "        return\n",
    "    else:\n",
    "        while not ExitFunc(user_input): #ExitFunc returns if user_input contains exit cue\n",
    "            if ExclamationFunc(user_input): #ExclamationFunc returns if user_input contains exclamation word\n",
    "                user_input = input()\n",
    "            else:#generate answer and take new input\n",
    "                ans=\"\" #stores final answer to be shown\n",
    "                match_found=0\n",
    "                for x in lis:#looping to find if a perfect match for user query exists in database\n",
    "                    if(user_input.lower() == x[0]):#perfect match\n",
    "                        ans=str(x[1])\n",
    "                        match_found=1\n",
    "                if(match_found == 1):\n",
    "                    user_input = input(ans+\"\\n\")#final answer is shown and a new user input is taken\n",
    "                else:\n",
    "                    t = word_tokenize(user_input.lower())\n",
    "                    M = np.zeros((1,MaxEncoder_Len,EncoderTokenLen),dtype='float32')\n",
    "                    for ts,x in enumerate(t):\n",
    "                        if x in InputFeatures_Dict:\n",
    "                            M[0,ts,InputFeatures_Dict[x]] = 1.0\n",
    "                    \n",
    "                    #Decode answer\n",
    "                    #Getting the output states to pass into the decoder\n",
    "                    StateValue = EncoderModel.predict(M)\n",
    "                    #Generating empty target sequence of length 1\n",
    "                    OutputSequence = np.zeros((1,1,DecoderTokenLen))\n",
    "                    #Setting the first token of target sequence with the start token\n",
    "                    OutputSequence[0,0,OutputFeatures_Dict['START']] = 1.0\n",
    "                    flag = 1\n",
    "                    while(flag == 1):\n",
    "                        #Predicting output tokens with probabilities and states\n",
    "                        OutputTokens,HiddenState,CellState = DecoderModel.predict([OutputSequence]+StateValue)\n",
    "                        #Choosing the one with highest probability\n",
    "                        i = np.argmax(OutputTokens[0, -1, :])\n",
    "                        t = RevOutputFeatures_Dict[i]\n",
    "                        ans = ans + \" \" + t\n",
    "                        #Stop if found the stop token\n",
    "                        if(t == 'STOP'):\n",
    "                            flag = 0\n",
    "                        OutputSequence = np.zeros((1, 1, DecoderTokenLen))\n",
    "                        OutputSequence[0,0,i] = 1.0\n",
    "                        #Update states\n",
    "                        StateValue=[HiddenState,CellState]\n",
    "                        #Remove <START> and <STOP> tokens from chatbot_response\n",
    "                        ans = ans.replace(\"START\",'')\n",
    "                        ans = ans.replace(\"STOP\",'')\n",
    "                    user_input = input(ans+\"\\n\")#final answer is shown and a new user input is taken\n",
    "                        \n",
    "\n",
    "#Method to check for exit commands\n",
    "def ExitFunc(r):\n",
    "    for e in exit_word:\n",
    "        if e in r:\n",
    "            print(\"okay , have a great day\")\n",
    "            return True\n",
    "    return False                  \n",
    "    \n",
    "#Method to check for exclamation\n",
    "def ExclamationFunc(r):\n",
    "    for e in exclamation_word:\n",
    "        if e in r:\n",
    "            print(\"okay , happy to help\")\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm Kolly. I can help you with Kolkata tourism. Ask me about historical locations & restaurants. Type `bye` to exit me. What would you like to know?\n",
      "hi\n",
      "hi there, how are you\n",
      "am fine\n",
      " no \n",
      "what\n",
      " i am a travel chatbot and can tell you about famous locations , food , restaurants in kolkata \n",
      "who are you\n",
      "i am a travel chatbot and can tell you about famous locations, food, restaurants in kolkata\n",
      "do you have a name?\n",
      " my name is kolly \n",
      "nice name\n",
      "okay , happy to help\n",
      "tell me where can i tour in kolkata?\n",
      " victoria memorial , howrah bridge , indian museum , fort william , birla planetarium , marble palace , st pauls cathedral , science city , birla mandir , eden gardens , jorasanko thakurbari , rabindra sarovar , shobhabazar rajbari , kalighat temple , botanical gardens , alipore zoo , prinsep ghat , eco park , nicco park , park street , south park street cemetery , national library , kumortuli , nandan , mothers wax museum , biswa bangla gate , salt lake stadium \n",
      "sounds nice\n",
      "okay , happy to help\n",
      "enlighten me about science city\n",
      " the science city of kolkata is the largest science centre in the indian subcontinent the main motto of the science city is the popularisation of science \n",
      "okay\n",
      "okay , happy to help\n",
      "tell me of some american restaurants?\n",
      " bengali , mughlai , north indian , chinese , american , fast food , cafe , buffet , seafood , vegetarian \n",
      "american?\n",
      " hard rock cafe , chillis , tgi fridays , macazzo , wild west tavern , uno chicago bar grill , saz , biggies burger , mcdonalds , kfc , subway , my big fat belly , mocambo , peter cat , steak factory \n",
      "where do i get chinese food?\n",
      " barbeque nation , sigree global grill , haka , wedesi , mainland china , 6 ballygunge place , ocean grill , flame n grill , chapter 2 , social kitchen , tung fong , itc sonar bangla , jw kitchen , itc royal bengal , rendezvous , chai break , mio amore restaurant , the wall \n",
      "no chinese?\n",
      " tung nam eating house , tung fong restaurant , mainland china , hong kong chinese restaurant , haka , golden joy restaurant , barbq , china town restaurant , big boss restaurant , golden dragon , beijing restaurant , royal jade , kim ling , krystal chopstick , chowman , chung wah , jimmys kitchen , hatari restaurant , asia kitchen , buddha bites \n",
      "okay\n",
      "okay , happy to help\n",
      "thanks\n",
      "okay , happy to help\n",
      "bye\n",
      "okay , have a great day\n"
     ]
    }
   ],
   "source": [
    "Initiate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot1 = plt.plot(history.history['loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
